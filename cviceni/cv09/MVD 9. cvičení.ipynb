{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVD 9. cvičení\n",
    "\n",
    "Dnešní cvičení nebude až tak obtížné. Cílem je seznámit se s HuggingFace a vyzkoušet si základní práci s BERT modelem.\n",
    "\n",
    "## 1. část - Seznámení s HuggingFace a modelem BERT\n",
    "\n",
    "Nainstalujte si Python knihovnu `transformers` a podívejte se na předtrénovaný [BERT model](https://huggingface.co/bert-base-uncased). Vyzkoušejte si unmasker s různými vstupy.\n",
    "\n",
    "<br>\n",
    "Pozn.: Použití BERT modelu vyžaduje zároveň PyTorch - postačí i cpu verze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'score': 0.44494354724884033,\n  'token': 3274,\n  'token_str': 'computer',\n  'sequence': \"hello i'm student of computer science\"},\n {'score': 0.19867515563964844,\n  'token': 2576,\n  'token_str': 'political',\n  'sequence': \"hello i'm student of political science\"},\n {'score': 0.02690133824944496,\n  'token': 2591,\n  'token_str': 'social',\n  'sequence': \"hello i'm student of social science\"},\n {'score': 0.009483722038567066,\n  'token': 3019,\n  'token_str': 'natural',\n  'sequence': \"hello i'm student of natural science\"},\n {'score': 0.00781850703060627,\n  'token': 6228,\n  'token_str': 'mechanical',\n  'sequence': \"hello i'm student of mechanical science\"}]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Hello I'm student of [MASK] science \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. část - BERT contextualized word embeddings\n",
    "\n",
    "BERT dokumentace obsahuje také návod jak použít tento model pro získání word embeddingů. Vyzkoušejte použití stejného slova v různém kontextu a podívejte se, jak se mění kosinova podobnost embeddingů v závislosti na kontextu daného slova.\n",
    "\n",
    "Podívejte se na výstup tokenizeru před vstupem do BERT modelu - kolik tokenů bylo vytvořeno pro větu \"Hello, this is Bert.\"? Zdůvodněte jejich počet.\n",
    "\n",
    "<br>\n",
    "Pozn.: Vyřešení předchozí otázky Vám pomůže zjistit, který vektor z výstupu pro cílové slovo použít."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1010,  2023,  2003, 14324,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0324,  0.2762, -0.2547,  ..., -0.2157,  0.1051,  0.6700],\n",
      "         [ 0.2507,  0.1135,  0.0737,  ...,  0.0563,  0.9311,  0.2921],\n",
      "         [-0.6409,  0.3774,  0.1627,  ..., -0.4889,  0.3352,  0.3493],\n",
      "         ...,\n",
      "         [ 0.7059,  0.0848,  0.5835,  ...,  0.2181,  0.4753, -0.1627],\n",
      "         [ 0.4089,  0.1187, -0.6125,  ...,  0.4456, -0.2722, -0.6050],\n",
      "         [ 0.7017,  0.1312, -0.4624,  ...,  0.3150, -0.5517, -0.3797]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8061, -0.4704, -0.9495,  0.6776,  0.7262, -0.1259,  0.7468,  0.2808,\n",
      "         -0.8649, -1.0000, -0.5832,  0.9513,  0.9563,  0.6321,  0.8333, -0.6898,\n",
      "         -0.3962, -0.5891,  0.3375,  0.1879,  0.5585,  1.0000, -0.0234,  0.3936,\n",
      "          0.5285,  0.9915, -0.7717,  0.8326,  0.9371,  0.6433, -0.4571,  0.3206,\n",
      "         -0.9771, -0.3148, -0.9538, -0.9836,  0.4486, -0.6102, -0.1161, -0.0485,\n",
      "         -0.7982,  0.3923,  1.0000, -0.1626,  0.4277, -0.3913, -1.0000,  0.3412,\n",
      "         -0.8008,  0.9609,  0.9324,  0.9156,  0.2875,  0.4524,  0.5262, -0.1898,\n",
      "         -0.0077,  0.2117, -0.3237, -0.5893, -0.7035,  0.5064, -0.8956, -0.8709,\n",
      "          0.9480,  0.8823, -0.2828, -0.4023, -0.1249, -0.0357,  0.8664,  0.2311,\n",
      "         -0.2360, -0.8207,  0.7712,  0.3621, -0.6670,  1.0000, -0.4636, -0.9401,\n",
      "          0.9188,  0.8666,  0.6473, -0.4458,  0.5596, -1.0000,  0.6905, -0.2075,\n",
      "         -0.9736,  0.4035,  0.6490, -0.4280,  0.6640,  0.6539, -0.5929, -0.4907,\n",
      "         -0.4152, -0.8886, -0.3679, -0.3044,  0.1687, -0.3308, -0.5909, -0.5050,\n",
      "          0.3962, -0.5722, -0.2406,  0.5259,  0.1252,  0.6381,  0.5140, -0.4631,\n",
      "          0.5050, -0.9342,  0.6998, -0.4649, -0.9803, -0.6419, -0.9752,  0.5920,\n",
      "         -0.3771, -0.3281,  0.9113, -0.1156,  0.5609, -0.1805, -0.9670, -1.0000,\n",
      "         -0.6712, -0.7426, -0.3027, -0.4494, -0.9498, -0.9532,  0.6162,  0.9157,\n",
      "          0.3056,  0.9998, -0.3709,  0.9196, -0.4657, -0.7417,  0.6497, -0.5852,\n",
      "          0.8306,  0.3428, -0.2502,  0.3005, -0.4458,  0.5831, -0.7860, -0.3459,\n",
      "         -0.8711, -0.8701, -0.4468,  0.8829, -0.6763, -0.9632, -0.1939, -0.2308,\n",
      "         -0.5258,  0.7361,  0.8084,  0.4858, -0.3945,  0.5979,  0.3446,  0.6499,\n",
      "         -0.8438, -0.3255,  0.5047, -0.3467, -0.9252, -0.9629, -0.4152,  0.5738,\n",
      "          0.9793,  0.6752,  0.4624,  0.8780, -0.4111,  0.8136, -0.9283,  0.9650,\n",
      "         -0.2895,  0.4963, -0.6334,  0.4473, -0.7362,  0.0574,  0.7703, -0.7257,\n",
      "         -0.7291, -0.3002, -0.5710, -0.3717, -0.8715,  0.2994, -0.2892, -0.3971,\n",
      "         -0.2336,  0.8729,  0.9168,  0.5265,  0.5153,  0.6739, -0.7876, -0.3644,\n",
      "          0.1761,  0.3120,  0.2167,  0.9818, -0.8115, -0.1918, -0.9003, -0.9670,\n",
      "          0.0011, -0.8414, -0.3581, -0.7141,  0.7738, -0.4068,  0.6720,  0.5522,\n",
      "         -0.8942, -0.6292,  0.4296, -0.5874,  0.5296, -0.3287,  0.8646,  0.9535,\n",
      "         -0.5921,  0.1809,  0.9202, -0.9526, -0.7929,  0.4100, -0.3523,  0.7817,\n",
      "         -0.7279,  0.9523,  0.9643,  0.7698, -0.8051, -0.8357, -0.6456, -0.7714,\n",
      "         -0.1679,  0.2782,  0.9335,  0.7140,  0.4386,  0.2702, -0.6046,  0.9722,\n",
      "         -0.8455, -0.9070, -0.8372, -0.1598, -0.9687,  0.8916,  0.3196,  0.5350,\n",
      "         -0.5227, -0.7722, -0.9179,  0.6186,  0.2504,  0.9292, -0.2864, -0.8385,\n",
      "         -0.5854, -0.8674, -0.0247, -0.2937, -0.5876,  0.0662, -0.8820,  0.5329,\n",
      "          0.5304,  0.5870, -0.9015,  0.9942,  1.0000,  0.9306,  0.8090,  0.8123,\n",
      "         -1.0000, -0.7452,  1.0000, -0.9935, -1.0000, -0.8634, -0.7036,  0.3091,\n",
      "         -1.0000, -0.3756, -0.0821, -0.8519,  0.7107,  0.9304,  0.9551, -1.0000,\n",
      "          0.7227,  0.8762, -0.6697,  0.9724, -0.5743,  0.9378,  0.5705,  0.4456,\n",
      "         -0.2859,  0.4081, -0.9593, -0.8013, -0.7324, -0.7614,  0.9994,  0.1981,\n",
      "         -0.6926, -0.7496,  0.5995, -0.1680, -0.1369, -0.9498, -0.4046,  0.6963,\n",
      "          0.7851,  0.1583,  0.4502, -0.5350,  0.4290,  0.1372,  0.0512,  0.6835,\n",
      "         -0.8519, -0.0904, -0.3367, -0.1985, -0.7404, -0.9614,  0.9237, -0.5183,\n",
      "          0.9507,  1.0000,  0.3739, -0.7230,  0.7212,  0.3963, -0.5966,  1.0000,\n",
      "          0.8210, -0.9406, -0.5755,  0.5774, -0.6535, -0.6747,  0.9991, -0.3255,\n",
      "         -0.8442, -0.5901,  0.9659, -0.9711,  0.9982, -0.8623, -0.9348,  0.9376,\n",
      "          0.8659, -0.7618, -0.5644,  0.2865, -0.7274,  0.3623, -0.8291,  0.7545,\n",
      "          0.4607, -0.1111,  0.7704, -0.6634, -0.6609,  0.3395, -0.6890, -0.4004,\n",
      "          0.9581,  0.6923, -0.3830,  0.1133, -0.3877, -0.7653, -0.9571,  0.6366,\n",
      "          1.0000, -0.3341,  0.8797, -0.4548, -0.2402,  0.0212,  0.6326,  0.6576,\n",
      "         -0.3929, -0.7504,  0.8430, -0.8949, -0.9746,  0.4846,  0.2296, -0.4096,\n",
      "          1.0000,  0.6484,  0.2525,  0.5063,  0.9903,  0.0135,  0.4470,  0.9316,\n",
      "          0.9590, -0.3132,  0.6320,  0.5897, -0.9256, -0.3832, -0.7024,  0.0948,\n",
      "         -0.8930, -0.0469, -0.9094,  0.9361,  0.9752,  0.5500,  0.3405,  0.8131,\n",
      "          1.0000, -0.8845,  0.5001,  0.4617,  0.4530, -0.9999, -0.7858, -0.4483,\n",
      "         -0.2967, -0.8869, -0.5625,  0.4548, -0.9319,  0.9141,  0.6719, -0.9659,\n",
      "         -0.9691, -0.3642,  0.7504,  0.2160, -0.9937, -0.6738, -0.5867,  0.7852,\n",
      "         -0.2483, -0.8765, -0.4975, -0.4982,  0.4854, -0.2624,  0.6510,  0.9110,\n",
      "          0.7319, -0.8521, -0.4206, -0.1306, -0.7015,  0.7953, -0.5991, -0.9576,\n",
      "         -0.4107,  1.0000, -0.5950,  0.9268,  0.5811,  0.3591, -0.1416,  0.3790,\n",
      "          0.9448,  0.3569, -0.7627, -0.9342,  0.3398, -0.4218,  0.6905,  0.7112,\n",
      "          0.7845,  0.7134,  0.9154,  0.1421, -0.1830,  0.0846,  0.9942, -0.1351,\n",
      "         -0.1715, -0.4080, -0.1155, -0.3924,  0.2200,  1.0000,  0.4000,  0.6068,\n",
      "         -0.9742, -0.9505, -0.8629,  1.0000,  0.7648, -0.5252,  0.6688,  0.6900,\n",
      "         -0.3324,  0.6379, -0.3490, -0.4457,  0.4395,  0.2535,  0.8804, -0.6544,\n",
      "         -0.9434, -0.6447,  0.5531, -0.9314,  1.0000, -0.6722, -0.3528, -0.5179,\n",
      "         -0.2409, -0.8186, -0.0805, -0.9552, -0.2983,  0.3526,  0.8962,  0.4271,\n",
      "         -0.6045, -0.7445,  0.9041,  0.8396, -0.9383, -0.9033,  0.8933, -0.9689,\n",
      "          0.6824,  1.0000,  0.5254,  0.4124,  0.3691, -0.2814,  0.5300, -0.4103,\n",
      "          0.7146, -0.8930, -0.3308, -0.2582,  0.3703, -0.1317, -0.5659,  0.6706,\n",
      "          0.3847, -0.6233, -0.7059, -0.1578,  0.4223,  0.8129, -0.3052, -0.3223,\n",
      "          0.1844, -0.1819, -0.7687, -0.4359, -0.4921, -1.0000,  0.4926, -1.0000,\n",
      "          0.5773,  0.4090, -0.2635,  0.7667,  0.6419,  0.7746, -0.6186, -0.8948,\n",
      "          0.3057,  0.6777, -0.3551, -0.6554, -0.4168,  0.4523, -0.1705,  0.1807,\n",
      "         -0.6849,  0.7671, -0.2999,  1.0000,  0.3212, -0.7605, -0.8989,  0.2636,\n",
      "         -0.3805,  1.0000, -0.6815, -0.9059,  0.5817, -0.8096, -0.7558,  0.4550,\n",
      "          0.0973, -0.8128, -0.9683,  0.8658,  0.6752, -0.6558,  0.4514, -0.3892,\n",
      "         -0.5373,  0.2056,  0.9353,  0.9724,  0.5796,  0.7317,  0.0659, -0.4655,\n",
      "          0.9437,  0.2882, -0.2157,  0.2376,  1.0000,  0.4471, -0.8914,  0.0073,\n",
      "         -0.9056, -0.3670, -0.8631,  0.4215,  0.3923,  0.8215, -0.3537,  0.8706,\n",
      "         -0.9152,  0.1014, -0.7092, -0.7420,  0.3933, -0.8105, -0.9562, -0.9599,\n",
      "          0.6793, -0.4957, -0.1240,  0.2895, -0.0115,  0.4837,  0.5421, -1.0000,\n",
      "          0.8899,  0.5308,  0.9417,  0.9117,  0.7765,  0.6199,  0.3963, -0.9655,\n",
      "         -0.9048, -0.3704, -0.4187,  0.5597,  0.6117,  0.8587,  0.4742, -0.5710,\n",
      "         -0.3821, -0.6230, -0.7326, -0.9805,  0.5655, -0.7559, -0.8022,  0.9389,\n",
      "         -0.0507, -0.2523, -0.2717, -0.8834,  0.6558,  0.6949,  0.1368,  0.1979,\n",
      "          0.4034,  0.6978,  0.9045,  0.9530, -0.8822,  0.6671, -0.7698,  0.5304,\n",
      "          0.9189, -0.9026,  0.2234,  0.5702, -0.3987,  0.3000, -0.3603, -0.8606,\n",
      "          0.8073, -0.3780,  0.5462, -0.5019, -0.0031, -0.5052, -0.2918, -0.7520,\n",
      "         -0.7740,  0.6739,  0.4578,  0.8086,  0.8816, -0.0947, -0.7377, -0.0778,\n",
      "         -0.8365, -0.8969,  0.8232, -0.0650, -0.3957,  0.7657, -0.0758,  0.9600,\n",
      "          0.3127, -0.3631, -0.3175, -0.5748,  0.7645, -0.6075, -0.6304, -0.5752,\n",
      "          0.7342,  0.4046,  1.0000, -0.8399, -0.9373, -0.4695, -0.4678,  0.6287,\n",
      "         -0.6788, -1.0000,  0.4589, -0.5846,  0.8331, -0.7872,  0.8987, -0.7148,\n",
      "         -0.9314, -0.3962,  0.6300,  0.8407, -0.5616, -0.5194,  0.5556, -0.4239,\n",
      "          0.9820,  0.7683, -0.5887, -0.0060,  0.6499, -0.8652, -0.7272,  0.8211]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Hello, this is Bert.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "output = model(**encoded_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Vizualizace slovních  embeddingů\n",
    "\n",
    "Vizualizujte slovní embeddingy - mění se jejich pozice v závislosti na kontextu tak, jak byste očekávali? Pokuste se vizualizovat i některá slova, ke kterým by se podle vás cílové slovo mělo po změně kontextu přiblížit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
