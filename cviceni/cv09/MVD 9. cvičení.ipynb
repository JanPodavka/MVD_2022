{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVD 9. cvičení\n",
    "\n",
    "Dnešní cvičení nebude až tak obtížné. Cílem je seznámit se s HuggingFace a vyzkoušet si základní práci s BERT modelem.\n",
    "\n",
    "## 1. část - Seznámení s HuggingFace a modelem BERT\n",
    "\n",
    "Nainstalujte si Python knihovnu `transformers` a podívejte se na předtrénovaný [BERT model](https://huggingface.co/bert-base-uncased). Vyzkoušejte si unmasker s různými vstupy.\n",
    "\n",
    "<br>\n",
    "Pozn.: Použití BERT modelu vyžaduje zároveň PyTorch - postačí i cpu verze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'score': 0.44494354724884033,\n  'token': 3274,\n  'token_str': 'computer',\n  'sequence': \"hello i'm student of computer science\"},\n {'score': 0.19867515563964844,\n  'token': 2576,\n  'token_str': 'political',\n  'sequence': \"hello i'm student of political science\"},\n {'score': 0.02690133824944496,\n  'token': 2591,\n  'token_str': 'social',\n  'sequence': \"hello i'm student of social science\"},\n {'score': 0.009483722038567066,\n  'token': 3019,\n  'token_str': 'natural',\n  'sequence': \"hello i'm student of natural science\"},\n {'score': 0.00781850703060627,\n  'token': 6228,\n  'token_str': 'mechanical',\n  'sequence': \"hello i'm student of mechanical science\"}]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Hello I'm student of [MASK] science \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. část - BERT contextualized word embeddings\n",
    "\n",
    "BERT dokumentace obsahuje také návod jak použít tento model pro získání word embeddingů. Vyzkoušejte použití stejného slova v různém kontextu a podívejte se, jak se mění kosinova podobnost embeddingů v závislosti na kontextu daného slova.\n",
    "\n",
    "Podívejte se na výstup tokenizeru před vstupem do BERT modelu - kolik tokenů bylo vytvořeno pro větu \"Hello, this is Bert.\"? Zdůvodněte jejich počet.\n",
    "\n",
    "<br>\n",
    "Pozn.: Vyřešení předchozí otázky Vám pomůže zjistit, který vektor z výstupu pro cílové slovo použít."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['its', 'mean', 'of', 'rows']\n",
      "tensor([[ 101, 2017, 2024, 2812, 2000, 2033,  102]]) tensor([[ 101, 2017, 2024, 2812, 2000, 2033,  102]])\n",
      "torch.Size([7, 768]) torch.Size([6, 768])\n",
      "torch.Size([768]) torch.Size([768])\n",
      "tensor([0.4805])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = [\"its mean of rows\",\n",
    "        \"mean by column\",\n",
    "        \"You are mean to me\",\n",
    "        \"its mean of rows\"\n",
    "]\n",
    "### tokens ###\n",
    "tokens = tokenizer.tokenize(text[0])\n",
    "print(tokens) # + [CLS]101 a [SEP]102\n",
    "### 1\n",
    "# tok1 = tokenizer(text[0], return_tensors='pt')\n",
    "# tok2 = tokenizer(text[1], return_tensors='pt')\n",
    "# 2\n",
    "tok1 = tokenizer(text[2], return_tensors='pt')\n",
    "tok2 = tokenizer(text[3], return_tensors='pt')\n",
    "print(tok1['input_ids'],tok1['input_ids']) #1 and 5 pos\n",
    "## 1\n",
    "# idx1 = 2\n",
    "# idx2 = 1\n",
    "# 2\n",
    "idx1 = 3\n",
    "idx2 = 2\n",
    "###\n",
    "## fit model\n",
    "with torch.no_grad():\n",
    "    out1 = model(**tok1)\n",
    "    out2 = model(**tok2)\n",
    "\n",
    "## get last hidden layer\n",
    "states1 = out1.last_hidden_state.squeeze()\n",
    "states2 = out2.last_hidden_state.squeeze()\n",
    "print(states1.shape,states2.shape)\n",
    "\n",
    "## get embedings\n",
    "emb1 = states1[idx1]\n",
    "emb2 = states1[idx2]\n",
    "print(emb1.shape,emb2.shape)\n",
    "\n",
    "\n",
    "cs = torch.cosine_similarity(emb1.reshape(1,-1), emb2.reshape(1,-1))\n",
    "print(cs) ## cosine == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Vizualizace slovních  embeddingů\n",
    "\n",
    "Vizualizujte slovní embeddingy - mění se jejich pozice v závislosti na kontextu tak, jak byste očekávali? Pokuste se vizualizovat i některá slova, ke kterým by se podle vás cílové slovo mělo po změně kontextu přiblížit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
