{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MVD 5. cvičení\n",
    "\n",
    "## 1. část - TF-IDF s word embeddingy\n",
    "\n",
    "V minulém cvičení bylo za úkol implementovat TF-IDF algoritmus nad datasetem z Kagglu. Dnešní cvičení je rozšířením této úlohy s použitím word embeddingů. Lze použít předtrénované GloVe embeddingy ze 3. cvičení, nebo si v případě zájmu můžete vyzkoušet práci s Word2Vec od Googlu (najdete [zde](https://code.google.com/archive/p/word2vec/)).\n",
    "\n",
    "Cvičení by mělo obsahovat následující části:\n",
    "- Načtení článků a embeddingů\n",
    "- Výpočet document vektorů pomocí TF-IDF a word embeddingů \n",
    "    - Pro výpočet TF-IDF využijte [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) z knihovny sklearn\n",
    "    - Vážený průměr GloVe / Word2Vec vektorů\n",
    "\n",
    "<center>\n",
    "$\n",
    "doc\\_vector = \\frac{1}{|d|} \\sum\\limits_{w \\in d} TF\\_IDF(w) glove(w)\n",
    "$\n",
    "</center>\n",
    "\n",
    "- Dotaz bude transformován stejně jako dokument\n",
    "\n",
    "- Výpočet relevance pomocí kosinové podobnosti\n",
    "<center>\n",
    "$\n",
    "score(q,d) = cos\\_sim(query\\_vector, doc\\_vector)\n",
    "$\n",
    "</center>\n",
    "\n",
    "### Načtení článků"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\podav\\anaconda3\\envs\\data_science\\lib\\site-packages\\spacy\\language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                 title  \\\n0    chatbots were the next big thing what happened...   \n1    python for data science 8 concepts you may hav...   \n2    automated feature engineering in python toward...   \n3    machine learning how to go from zero to hero f...   \n4     reinforcement learning from scratch insight data   \n..                                                 ...   \n332  you can build a neural network in javascript e...   \n333  artificial intelligence ai in 2018 and beyond ...   \n334  spiking neural networks the next generation of...   \n335  surprise neurons are now more complex than we ...   \n336   wth does a neural network even learn a newcom...   \n\n                                                  text  \n0    oh how the headlines blared chatbots were the ...  \n1    if you ve ever found yourself looking up the s...  \n2    machine learning is increasingly moving from h...  \n3    if your understanding of a i and machine learn...  \n4    want to learn about applied artificial intelli...  \n..                                                 ...  \n332  click here to share this article on linkedin s...  \n333  these are my opinions on where deep neural net...  \n334  everyone who has been remotely tuned in to rec...  \n335  one of the biggest misconceptions around is th...  \n336  i believe we all have that psychologist philos...  \n\n[337 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>chatbots were the next big thing what happened...</td>\n      <td>oh how the headlines blared chatbots were the ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>python for data science 8 concepts you may hav...</td>\n      <td>if you ve ever found yourself looking up the s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>automated feature engineering in python toward...</td>\n      <td>machine learning is increasingly moving from h...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>machine learning how to go from zero to hero f...</td>\n      <td>if your understanding of a i and machine learn...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>reinforcement learning from scratch insight data</td>\n      <td>want to learn about applied artificial intelli...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>332</th>\n      <td>you can build a neural network in javascript e...</td>\n      <td>click here to share this article on linkedin s...</td>\n    </tr>\n    <tr>\n      <th>333</th>\n      <td>artificial intelligence ai in 2018 and beyond ...</td>\n      <td>these are my opinions on where deep neural net...</td>\n    </tr>\n    <tr>\n      <th>334</th>\n      <td>spiking neural networks the next generation of...</td>\n      <td>everyone who has been remotely tuned in to rec...</td>\n    </tr>\n    <tr>\n      <th>335</th>\n      <td>surprise neurons are now more complex than we ...</td>\n      <td>one of the biggest misconceptions around is th...</td>\n    </tr>\n    <tr>\n      <th>336</th>\n      <td>wth does a neural network even learn a newcom...</td>\n      <td>i believe we all have that psychologist philos...</td>\n    </tr>\n  </tbody>\n</table>\n<p>337 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "lemmatizer = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # NLTK\n",
    "\n",
    "\n",
    "def normalize_data(db):\n",
    "    # set all to lowercase\n",
    "    db['text'] = db['text'].str.lower()\n",
    "    db['title'] = db['title'].str.lower()\n",
    "\n",
    "    # delete all special characters\n",
    "    db['text'] = db['text'].str.replace(r'\\W', ' ', regex=True)\n",
    "    db['title'] = db['title'].str.replace(r'\\W', ' ', regex=True)\n",
    "    # delete multiple spaces\n",
    "    db['text'] = db['text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    db['title'] = db['title'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    return db\n",
    "\n",
    "\n",
    "#data load and normalize\n",
    "df = pandas.read_csv('articles.csv')\n",
    "df = df[['title', 'text']]\n",
    "df_norm = normalize_data(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Načtení embeddingů"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "the\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('glove.6B/glove.6B.50D.txt', encoding='utf8') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        data.append(line)\n",
    "word = []\n",
    "word2idx = {}\n",
    "vec = np.zeros((len(data),len(data[0].split(' '))-1))\n",
    "for i,item in enumerate(data):\n",
    "    splited = item.replace('\\n','').split(' ')\n",
    "    word.append(splited[0])\n",
    "    vec[i,:] = np.asarray(splited[1:])\n",
    "    word2idx[splited[0]] = i\n",
    "\n",
    "print(vec[0])\n",
    "print(word[0])\n",
    "print(word2idx['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def cossim(a,b):\n",
    "    return (a @ b) / ((np.linalg.norm(a)*np.linalg.norm(b)) + 0.0000001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def tf(words):\n",
    "    out = {}\n",
    "    for word in words:\n",
    "        if word not in out:\n",
    "            out[word] = 1\n",
    "        else:\n",
    "            out[word] += 1\n",
    "    return out\n",
    "\n",
    "def tf_idf(q,d,ii):\n",
    "\n",
    "    M = len(d) #num of all documents\n",
    "    q_lem = \" \".join([token.lemma_ for token in lemmatizer(q)]).split(' ')\n",
    "    freq = tf(q_lem) # how many times is each word in query\n",
    "    out = []\n",
    "    for doc in d:\n",
    "        scores = 0\n",
    "        wd = tf(\" \".join([token.lemma_ for token in lemmatizer(doc)]).split(' ')) #how many times is each word in specific document\n",
    "        for word in q_lem:\n",
    "            if word in wd:\n",
    "                scores += freq[word] * wd[word] *  np.log((M+1) / len(set(ii[word])) )\n",
    "        out.append(scores)\n",
    "    return  out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def inverted_index(texts):\n",
    "    text_dic = {}\n",
    "    for i,line in enumerate(texts):\n",
    "        for word in \" \".join([token.lemma_ for token in lemmatizer(line)]).split(' '):\n",
    "            if word not in text_dic:\n",
    "                text_dic[word] = [i]\n",
    "            else:\n",
    "                text_dic[word].append(i)\n",
    "\n",
    "    return text_dic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "text_ii = inverted_index(df_norm['text'])\n",
    "title_ii = inverted_index(df_norm['title'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TF-IDF + Word2Vec a vytvoření doc vektorů"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "[[[3816.19675835 -139.750478    770.91491913 ...  270.71149613\n",
      "   -393.5165917  1046.41952335]]\n",
      "\n",
      " [[1085.5509731   479.69497679  404.14137342 ...  831.42300975\n",
      "   -393.33614586  316.05754072]]\n",
      "\n",
      " [[ 555.44113193  195.83241984  164.85683147 ... 1060.510761\n",
      "     18.73093134 -194.36356185]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2307.72608249  506.15305898 3265.90197228 ... -606.73325783\n",
      "   -748.65024631 -628.32866823]]\n",
      "\n",
      " [[1524.76296513   43.94033426  521.37592935 ...  195.54004851\n",
      "   -601.59704454  353.86521366]]\n",
      "\n",
      " [[2503.90234419 3346.04191264 1203.48764138 ... 2224.78129827\n",
      "   1175.39957819 2963.43665825]]]\n"
     ]
    }
   ],
   "source": [
    "def doc_vec(d,ii):\n",
    "    doc_v = np.zeros((len(d),1,50))\n",
    "    query_vec = np.zeros((1,50))\n",
    "    for i,dv in enumerate(d):\n",
    "        d1_vec = np.zeros((1,50))\n",
    "        di = np.array(tf_idf(dv,d,ii)).sum(axis = 0)\n",
    "        for w in dv.split(\" \"):\n",
    "            if w in word2idx:\n",
    "                d1_vec += vec[word2idx[w]] * di\n",
    "        query_vec /= len(dv.split(\" \"))\n",
    "        doc_v[i] = d1_vec\n",
    "        print(i)\n",
    "    return doc_v\n",
    "\n",
    "d_vec = doc_vec(df_norm['title'],title_ii)\n",
    "print(d_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -48.28642132 -114.87699405  263.30144729  -34.85819443   28.15154816\n",
      "   128.53535078  -28.05336303 -232.95894108  -37.97448448   95.27181213\n",
      "    57.94754734 -117.47802273  -92.28301899  -58.50592591 -214.26793548\n",
      "   -44.75310992 -209.86734907  470.05982582 -104.89031137 -223.32267358\n",
      "   235.95310282  195.40060415  -76.35850314   75.64537188  312.5953846\n",
      "  -420.06441488 -304.61407369 -264.36193981  331.12764364 -175.93353586\n",
      "  1253.88225721  -96.86190128 -268.51948023 -190.30090023   49.89546052\n",
      "   290.31803348  122.44014715  208.55126311   99.49805982  109.69392858\n",
      "   272.63552347 -164.95314345 -257.00323717  331.12675617  154.58771327\n",
      "  -107.67416018  327.45644999  -58.91725682    4.20906798  165.75764451]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def query_vec(q,d,ii):\n",
    "    query_vec = np.zeros((1,50))\n",
    "    for w in q.split(\" \"):\n",
    "        if w in word2idx:\n",
    "            query_vec += vec[word2idx[w]] * np.array(tf_idf(w,d,ii)).sum(axis = 0)\n",
    "    query_vec /= len(q.split(\" \"))\n",
    "    return np.array(query_vec)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transformace dotazu a výpočet relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -6.10533147  -8.42828629  21.7225685   -0.87193982  -0.35134624\n",
      "   10.12982589  -0.72145852 -19.7847615   -3.2313815    8.45122462\n",
      "    5.56506736 -10.68801083  -7.8287766   -6.65396507 -18.50859228\n",
      "   -3.76011018 -17.80316582  39.06653678 -10.25425096 -16.25390679\n",
      "   19.45546041  14.74946449  -6.51509618   8.49808727  26.40632496\n",
      "  -34.95873162 -22.79594686 -23.42215637  26.72050866 -14.91365807\n",
      "  106.69365229  -5.87175197 -22.3921459  -14.57018479   3.6158006\n",
      "   24.21315467  10.37955324  17.99955362   7.50820361   6.74671778\n",
      "   25.11893168 -15.08138985 -20.59913838  25.57670735  14.56643153\n",
      "   -6.72564207  29.55293695  -5.83870106   2.33114702  13.55679168]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                 title  \\\n90   an intro to machine learning for designers ux ...   \n6    an intro to machine learning for designers ux ...   \n192  ultimate guide to leveraging nlp machine learn...   \n202  cheat sheets for ai neural networks machine le...   \n292  cheat sheets for ai neural networks machine le...   \n..                                                 ...   \n234  de la coope ration entre les hommes et les mac...   \n286  multi stream rnn concat rnn internal conv rnn ...   \n167  o grupo de estudo em deep learning de brasi li...   \n307  sema ntica desde informacio n desestructurada ...   \n23                              建议的程序员学习lda算法的步骤 蒸汽与魔法   \n\n                                                  text     score  \n90   there is an ongoing debate about whether or no...  0.620963  \n6    there is an ongoing debate about whether or no...  0.620963  \n192  code snippets and github included over the pas...  0.612662  \n202  over the past few months i have been collectin...  0.609367  \n292  over the past few months i have been collectin...  0.609367  \n..                                                 ...       ...  \n234  originally published at www cuberevue com on n...  0.298082  \n286  for the last two week i have been dying to imp...  0.286621  \n167  o grupo de estudo em deep learning de brasi li...  0.268719  \n307  detectar patrones es un nu cleo importante en ...  0.223020  \n23   这一阵为了工作上的关系 花了点时间学习了一下lda算法 说实话 对于我这个学cs而非学数学的...  0.000000  \n\n[337 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>90</th>\n      <td>an intro to machine learning for designers ux ...</td>\n      <td>there is an ongoing debate about whether or no...</td>\n      <td>0.620963</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>an intro to machine learning for designers ux ...</td>\n      <td>there is an ongoing debate about whether or no...</td>\n      <td>0.620963</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>ultimate guide to leveraging nlp machine learn...</td>\n      <td>code snippets and github included over the pas...</td>\n      <td>0.612662</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>cheat sheets for ai neural networks machine le...</td>\n      <td>over the past few months i have been collectin...</td>\n      <td>0.609367</td>\n    </tr>\n    <tr>\n      <th>292</th>\n      <td>cheat sheets for ai neural networks machine le...</td>\n      <td>over the past few months i have been collectin...</td>\n      <td>0.609367</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>234</th>\n      <td>de la coope ration entre les hommes et les mac...</td>\n      <td>originally published at www cuberevue com on n...</td>\n      <td>0.298082</td>\n    </tr>\n    <tr>\n      <th>286</th>\n      <td>multi stream rnn concat rnn internal conv rnn ...</td>\n      <td>for the last two week i have been dying to imp...</td>\n      <td>0.286621</td>\n    </tr>\n    <tr>\n      <th>167</th>\n      <td>o grupo de estudo em deep learning de brasi li...</td>\n      <td>o grupo de estudo em deep learning de brasi li...</td>\n      <td>0.268719</td>\n    </tr>\n    <tr>\n      <th>307</th>\n      <td>sema ntica desde informacio n desestructurada ...</td>\n      <td>detectar patrones es un nu cleo importante en ...</td>\n      <td>0.223020</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>建议的程序员学习lda算法的步骤 蒸汽与魔法</td>\n      <td>这一阵为了工作上的关系 花了点时间学习了一下lda算法 说实话 对于我这个学cs而非学数学的...</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>337 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.7\n",
    "qs = 'coursera vs udacity machine learning'\n",
    "sim_title = []\n",
    "q_vec = query_vec(qs,df_norm['title'],title_ii)\n",
    "print(q_vec)\n",
    "for i in range(d_vec.shape[0]):\n",
    "    sim_title.append(cossim(np.squeeze(q_vec),np.squeeze(d_vec[i,:])))\n",
    "\n",
    "#np.array(tf_idf(qs, df_norm['text'], text_ii))\n",
    "test = list(alpha * np.array(sim_title))\n",
    "\n",
    "df_norm['score'] = test\n",
    "sorted_df = df_norm.sort_values(by='score', ascending=False)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bonus - Našeptávání\n",
    "\n",
    "Bonusem dnešního cvičení je našeptávání pomocí rekurentních neuronových sítí. Úkolem je vytvořit jednoduchou rekurentní neuronovou síť, která bude generovat text (character-level přístup). \n",
    "\n",
    "Optimální je začít po dokončení cvičení k předmětu ANS, kde se tato úloha řeší. \n",
    "\n",
    "Dataset pro učení vaší neuronové sítě naleznete na stránkách [Yahoo research](https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&guccounter=1), lze využít např. i větší [Kaggle dataset](https://www.kaggle.com/c/yandex-personalized-web-search-challenge/data) nebo vyhledat další dataset na [Google DatasetSearch](https://datasetsearch.research.google.com/).\n",
    "\n",
    "Vstupem bude rozepsaný dotaz a výstupem by měly být alespoň 3 dokončené dotazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}